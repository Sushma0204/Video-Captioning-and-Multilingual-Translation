{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c043aa3a-6501-4119-a25f-c4d1324e10e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yashp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "377eee3f-7b39-4d6f-aca2-f443900b887b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             english                 telugu\n",
      "0              Hello                    హలో\n",
      "1      Are you good?       మీరు బాగున్నారా?\n",
      "2         I am happy  నేను సంతోషంగా ఉన్నాను\n",
      "3       how are you?      మీరు ఎలా ఉన్నారు?\n",
      "4          I am good        నేను భాగున్నాను\n",
      "5      Are you good?       మీరు బాగున్నారా?\n",
      "6         I am angry    నేను కోపంగా ఉన్నాను\n",
      "7           I am sad  నేను విచారంగా ఉన్నాను\n",
      "8  This is my friend      ఇది నా స్నేహితుడు\n",
      "9    This is my home           ఇది నా ఇల్లు\n",
      "Index(['english', 'telugu'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "filepath = r\"C:\\Users\\yashp\\en_te.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "print(df.head(10))  # Check initial data\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e87ee88-850f-4857-acfc-b0b9b9419c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "english    0\n",
       "telugu     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2604e60e-f622-44c6-8f1e-34cf8d1af7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0ec6dd-500f-428b-9ee7-a780e513d1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "english    0\n",
       "telugu     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40a961d7-c25c-489d-b939-4fa15b073968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>telugu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello</td>\n",
       "      <td>హలో</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are you good?</td>\n",
       "      <td>మీరు బాగున్నారా?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am happy</td>\n",
       "      <td>నేను సంతోషంగా ఉన్నాను</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how are you?</td>\n",
       "      <td>మీరు ఎలా ఉన్నారు?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I am good</td>\n",
       "      <td>నేను భాగున్నాను</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         english                 telugu\n",
       "0          Hello                    హలో\n",
       "1  Are you good?       మీరు బాగున్నారా?\n",
       "2     I am happy  నేను సంతోషంగా ఉన్నాను\n",
       "3   how are you?      మీరు ఎలా ఉన్నారు?\n",
       "4      I am good        నేను భాగున్నాను"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eafe7c3-01e3-405f-876d-97349810eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "'''def load_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.columns = [\"English\", \"Marathi\"]\n",
    "    df.dropna(inplace=True)\n",
    "    return df'''\n",
    "\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath)  # Load the CSV without renaming columns\n",
    "    df = df[['english', 'telugu']]  # Keep only relevant columns\n",
    "    df.dropna(inplace=True)  # Remove any missing values\n",
    "    return df\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# Preprocess dataset\n",
    "def preprocess_data(df):\n",
    "    df[\"english\"] = df[\"english\"].apply(clean_text)\n",
    "    df[\"telugu\"] = df[\"telugu\"]\n",
    "    return df\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_text(df):\n",
    "    engHT_tokens = [word_tokenize(sent) for sent in df['english']]\n",
    "    TL_tokens = [word_tokenize(sent) for sent in df['telugu']]\n",
    "    return engHT_tokens, TL_tokens\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(tokenized_texts):\n",
    "    vocab = {word for sentence in tokenized_texts for word in sentence}\n",
    "    vocab = {word: idx + 1 for idx, word in enumerate(vocab)}\n",
    "    vocab['<PAD>'] = 0\n",
    "    return vocab\n",
    "\n",
    "# Encode sentences\n",
    "def encode_sentences(sentences, vocab, max_len):\n",
    "    encoded = [[vocab.get(word, 0) for word in sent] for sent in sentences]\n",
    "    padded = [sent + [0] * (max_len - len(sent)) if len(sent) < max_len else sent[:max_len] for sent in encoded]\n",
    "    return np.array(padded)\n",
    "\n",
    "# PyTorch Dataset Class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# LSTM Model Class\n",
    "class LSTMTranslator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
    "        super(LSTMTranslator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True, dropout=0.6, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.dropout = nn.Dropout(0.6)  # Higher dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)  # Apply dropout before FC\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Load and process data\n",
    "filepath = r\"C:\\Users\\yashp\\en_te.csv\"\n",
    "df = load_data(filepath)\n",
    "df = preprocess_data(df)\n",
    "engHT_tokens, TL_tokens = tokenize_text(df)\n",
    "engHT_vocab = build_vocab(engHT_tokens)\n",
    "TL_vocab = build_vocab(TL_tokens)\n",
    "\n",
    "# Set max sequence length\n",
    "max_len = 10\n",
    "X = encode_sentences(engHT_tokens, engHT_vocab, max_len)\n",
    "y = encode_sentences(TL_tokens, TL_vocab, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9306237-dcf5-468d-b116-f42333d8a295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>telugu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello</td>\n",
       "      <td>హలో</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>are you good</td>\n",
       "      <td>మీరు బాగున్నారా?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am happy</td>\n",
       "      <td>నేను సంతోషంగా ఉన్నాను</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how are you</td>\n",
       "      <td>మీరు ఎలా ఉన్నారు?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am good</td>\n",
       "      <td>నేను భాగున్నాను</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        english                 telugu\n",
       "0         hello                    హలో\n",
       "1  are you good       మీరు బాగున్నారా?\n",
       "2    i am happy  నేను సంతోషంగా ఉన్నాను\n",
       "3   how are you      మీరు ఎలా ఉన్నారు?\n",
       "4     i am good        నేను భాగున్నాను"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c010a50-6ca2-4ccb-a93a-f707b2937477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>telugu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10200</th>\n",
       "      <td>that was how he came to win 1 million</td>\n",
       "      <td>అతను million 1 మిలియన్ గెలిచాడు.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10201</th>\n",
       "      <td>jim liked driving around town with his hazard ...</td>\n",
       "      <td>జిమ్ తన ప్రమాదకర లైట్లతో పట్టణం చుట్టూ డ్రైవిం...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10202</th>\n",
       "      <td>so long and thanks for the fish</td>\n",
       "      <td>చాలా కాలం మరియు చేపలకు ధన్యవాదాలు.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10203</th>\n",
       "      <td>he barked orders at his daughters but they jus...</td>\n",
       "      <td>అతను తన కుమార్తెల వద్ద ఆదేశాలను మొరాయిస్తాడు, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10204</th>\n",
       "      <td>barking dogs and screaming toddlers have the u...</td>\n",
       "      <td>మొరిగే కుక్కలు మరియు అరుస్తూ పసిబిడ్డలు స్నేహప...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 english  \\\n",
       "10200              that was how he came to win 1 million   \n",
       "10201  jim liked driving around town with his hazard ...   \n",
       "10202                    so long and thanks for the fish   \n",
       "10203  he barked orders at his daughters but they jus...   \n",
       "10204  barking dogs and screaming toddlers have the u...   \n",
       "\n",
       "                                                  telugu  \n",
       "10200                   అతను million 1 మిలియన్ గెలిచాడు.  \n",
       "10201  జిమ్ తన ప్రమాదకర లైట్లతో పట్టణం చుట్టూ డ్రైవిం...  \n",
       "10202                 చాలా కాలం మరియు చేపలకు ధన్యవాదాలు.  \n",
       "10203  అతను తన కుమార్తెల వద్ద ఆదేశాలను మొరాయిస్తాడు, ...  \n",
       "10204  మొరిగే కుక్కలు మరియు అరుస్తూ పసిబిడ్డలు స్నేహప...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7278ef8-a930-4118-8f75-481dda83f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0cac517-0734-4c25-8b1e-3809612b5ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare DataLoader\n",
    "train_dataset = TranslationDataset(X_train, y_train)\n",
    "val_dataset = TranslationDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eaa9bd7-8c1a-420e-aa3d-a56dcbb62c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashp\\anaconda3\\envs\\FER_TL\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMTranslator(len(engHT_vocab), 128, 512, len(TL_vocab)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f408202-3b72-445e-b526-102eeddb61ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Device type: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\") # Get the name of the CUDA device\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"Device type: {device.type}\") # Print the type of device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e483a4-4e1d-4b6f-960c-91537fc3a84a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashp\\anaconda3\\envs\\FER_TL\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 6.6576, Val Loss: 5.9517\n",
      "Epoch 2, Train Loss: 5.1750, Val Loss: 4.7533\n",
      "Epoch 3, Train Loss: 3.8455, Val Loss: 4.0729\n",
      "Epoch 4, Train Loss: 2.9693, Val Loss: 3.7880\n",
      "Epoch 5, Train Loss: 2.3411, Val Loss: 3.6356\n",
      "Epoch 6, Train Loss: 1.8358, Val Loss: 3.5870\n",
      "Epoch 7, Train Loss: 1.4651, Val Loss: 3.5867\n",
      "Epoch 8, Train Loss: 1.2107, Val Loss: 3.6060\n",
      "Epoch 9, Train Loss: 1.0358, Val Loss: 3.6045\n",
      "Epoch 10, Train Loss: 0.7920, Val Loss: 3.5917\n",
      "Epoch 11, Train Loss: 0.6833, Val Loss: 3.6191\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "epochs = 30\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float(\"inf\")\n",
    "patience, patience_counter = 5, 0\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=2, factor=0.5, verbose=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output.view(-1, output.shape[-1]), y_batch.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output.view(-1, output.shape[-1]), y_batch.view(-1))\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    scheduler.step(val_losses[-1])  # Adjust learning rate\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_losses[-1] < best_val_loss:\n",
    "        best_val_loss = val_losses[-1]\n",
    "        torch.save(model.state_dict(), \"best_translator_model.pth\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e3f3cb-eb1f-4712-9fb9-b28d5a96afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e5afa-700c-44a0-99e6-dc316e350851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "torch.save(model.state_dict(), \"translator_model_English_to_telugu.pth\")\n",
    "pickle.dump(engHT_vocab, open(\"engHT_vocab.pkl\", \"wb\"))\n",
    "pickle.dump(TL_vocab, open(\"TL_vocab.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f881119-48ca-481b-8526-aa3ef96c8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, engHT_vocab, TL_vocab, max_len=10, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    encoded = [engHT_vocab.get(word, 0) for word in tokens]\n",
    "    padded = encoded + [0] * (max_len - len(encoded)) if len(encoded) < max_len else encoded[:max_len]\n",
    "    input_tensor = torch.tensor([padded], dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    \n",
    "    predicted_tokens = output.argmax(2).squeeze().tolist()\n",
    "    rev_TL_vocab = {idx: word for word, idx in TL_vocab.items()}\n",
    "    translated_sentence = \" \".join([rev_TL_vocab.get(idx, \"\") for idx in predicted_tokens])\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422597f-1e7e-41e6-bca4-e80d59ef4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "loaded_model = LSTMTranslator(len(engHT_vocab), 128, 512, len(TL_vocab)).to(device)\n",
    "loaded_model.load_state_dict(torch.load(\"translator_model_English_to_telugu.pth\"))\n",
    "loaded_model.eval()\n",
    "\n",
    "example_sentence = \"Thank You\"\n",
    "predicted_translation = translate_sentence(loaded_model, example_sentence, engHT_vocab, TL_vocab, device=device)\n",
    "print(f\"Translation: {predicted_translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2d23c-4eab-40ab-89a6-d76eef8d753e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
